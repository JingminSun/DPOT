name: NS2d
file: train_temporal_parallel.py


dataset: allv2
num_gpus: 2
use_writer: True
res: 128
modes: 20
width: 32
out_layer_dim: 32
lr: 0.001
lr_method: cycle
step_size: 100
step_gamma: 0.5
epochs: 1000
warmup_epochs: 200
noise_scale: 0.0
T_ar: 1
use_ln: 0
normalize: 0
T_bundle: 1
grad_clip: 10000.0



train_paths: [
        'ns2d_pdb_M1_eta1e-1_zeta1e-1',
        'ns2d_pdb_M1_eta1e-2_zeta1e-2',
        'ns2d_pdb_M1e-1_eta1e-1_zeta1e-1',
        'ns2d_pdb_M1e-1_eta1e-2_zeta1e-2',
        'ns2d_pdb_M1e-1_eta1e-8_zeta1e-8_turb_128',
        'ns2d_pdb_M1_eta1e-8_zeta1e-8_turb_128',
        'ns2d_pdb_M1e-1_eta1e-8_zeta1e-8_rand_128',
        'ns2d_pdb_M1_eta1e-8_zeta1e-8_rand_128',
        'ns2d_pdb_incom',
        'swe_pdb',
        'ns2d_cond_pda'
    ]
#  [
#'ns2d_pdb_M1_eta1e-1_zeta1e-1','ns2d_pdb_M1_eta1e-2_zeta1e-2','ns2d_pdb_M1e-1_eta1e-1_zeta1e-1','ns2d_pdb_M1e-1_eta1e-2_zeta1e-2','swe_pdb', 'ns2d_cond_pda'
#]
test_paths: [
        'ns2d_pdb_M1_eta1e-1_zeta1e-1',
        'ns2d_pdb_M1_eta1e-2_zeta1e-2',
        'ns2d_pdb_M1e-1_eta1e-1_zeta1e-1',
        'ns2d_pdb_M1e-1_eta1e-2_zeta1e-2',
        'ns2d_pdb_M1e-1_eta1e-8_zeta1e-8_turb_128',
        'ns2d_pdb_M1_eta1e-8_zeta1e-8_turb_128',
        'ns2d_pdb_M1e-1_eta1e-8_zeta1e-8_rand_128',
        'ns2d_pdb_M1_eta1e-8_zeta1e-8_rand_128',
        'ns2d_pdb_incom',
        'swe_pdb',
        'ns2d_cond_pda'
    ]
#  [
#'ns2d_pdb_M1_eta1e-1_zeta1e-1','ns2d_pdb_M1_eta1e-2_zeta1e-2','ns2d_pdb_M1e-1_eta1e-1_zeta1e-1','ns2d_pdb_M1e-1_eta1e-2_zeta1e-2','swe_pdb', 'ns2d_cond_pda'
#]
ntrain_list: [
        8000,
        8000,
        8000,
        8000,
        800,
        800,
        800,
        800,
        876,
        800,
        2496
    ]








##########################################################
########## Medium model
##########################################################
model: DPOT
#num_gpus: 1
opt: adam
comment: 'M'
tasks:
  normalize: [0]
  res: [128]
  patch_size: [8]
  noise_scale: [0.0005]
  modes: [32]
  width: [1024]
  mlp_ratio: [4]
  n_blocks: [8]
  n_layers: [12]
  lr: [0.001]
  epochs: [1000]
  lr_method: [cycle]
  warmup_epochs: [200]
  T_ar: [1]
  T_bundle: [1]
  beta1: [0.9]
  beta2: [0.9]
  batch_size: [20]


