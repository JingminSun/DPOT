name: NS2d
file: train_temporal_parallel.py

dataset: ns2d
use_writer: True
res: 128
modes: 20
width: 32
out_layer_dim: 32
lr: 0.001
lr_method: cycle
step_size: 100
step_gamma: 0.5
epochs: 1000
warmup_epochs: 200
noise_scale: 0.0
T_ar: 5
use_ln: 0
normalize: 0
T_bundle: 1
grad_clip: 10000.0




train_paths: [
'ns2d_pdb_M1_eta1e-1_zeta1e-1','ns2d_pdb_M1_eta1e-2_zeta1e-2','ns2d_pdb_M1e-1_eta1e-1_zeta1e-1','ns2d_pdb_M1e-1_eta1e-2_zeta1e-2','swe_pdb', 'ns2d_cond_pda'
]
test_paths: [
'ns2d_pdb_M1_eta1e-1_zeta1e-1','ns2d_pdb_M1_eta1e-2_zeta1e-2','ns2d_pdb_M1e-1_eta1e-1_zeta1e-1','ns2d_pdb_M1e-1_eta1e-2_zeta1e-2','swe_pdb', 'ns2d_cond_pda'
]
ntrain_list: [8000,8000,8000,8000,800,2496]



############################################################
########## DPOT for multiple PDE datasets pretraining (Small)
############################################################
model: DPOT
num_gpus: 2
opt: adam
comment: 'S'
tasks:
  normalize: [0]
  res: [128]
  patch_size: [8]
  noise_scale: [0.0005]
  modes: [32]
  width: [1024]
  mlp_ratio: [1]
  n_blocks: [8]
  n_layers: [6]
  lr: [0.001]
  epochs: [1000]
  lr_method: [cycle]
  warmup_epochs: [200]
  T_ar: [5]
  T_bundle: [5]
  beta1: [0.9]
  beta2: [0.9]
  batch_size: [20]




